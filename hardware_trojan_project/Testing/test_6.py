import pandas as pd
import numpy as np
import torch
import pickle
import torchvision.transforms as transforms
from torchvision import models
import torch.nn as nn
from PIL import Image
from pathlib import Path

# ==================== ÿ™ŸÜÿ∏€åŸÖÿßÿ™ Ÿà ŸÖÿ≥€åÿ±Ÿáÿß ====================
PROJECT_ROOT = Path(r"F:\Amin_Projects\University\VHDL\hardware_trojan_project")
MODELS_DIR = PROJECT_ROOT / "colab_outputs" / "models"
FEATURES_PATH = PROJECT_ROOT / "test_features.npy"
TEST_IMAGE_PATH = PROJECT_ROOT / "test_image_c17_trojan.png"
device = torch.device('cpu')

print("="*80)
print("TEST 6: FINAL VALIDATION REPORT")
print("="*80 + "\n")

# 1. ÿ®ÿ±ÿ±ÿ≥€å Ÿàÿ¨ŸàÿØ ŸÅÿß€åŸÑ‚ÄåŸáÿß
if not FEATURES_PATH.exists() or not TEST_IMAGE_PATH.exists():
    print("‚ùå Error: Required files not found. Run previous tests first.")
    exit()

# 2. ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ÿØÿßÿØŸá‚ÄåŸáÿß
test_features = np.load(FEATURES_PATH)
class_names = {0: "Low", 1: "Medium", 2: "High"}
expected_class = "High"
predictions = {}

print("üîÑ Loading Models & Running Predictions...")

# --- ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å Ÿà Ÿæ€åÿ¥‚Äåÿ®€åŸÜ€å CNN ---
try:
    cnn_model = models.resnet18(weights=None)
    cnn_model.fc = nn.Linear(cnn_model.fc.in_features, 3)
    state_dict = torch.load(MODELS_DIR / "resnet18_best.pth", map_location=device, weights_only=False)
    cnn_model.load_state_dict(state_dict)
    cnn_model.eval()
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    img = Image.open(TEST_IMAGE_PATH).convert('RGB')
    img_tensor = transform(img).unsqueeze(0)
    
    with torch.no_grad():
        output = cnn_model(img_tensor)
        probs = torch.softmax(output, dim=1).numpy()[0]
        pred_idx = np.argmax(probs)
        
    predictions['CNN'] = {
        'class': class_names[pred_idx],
        'confidence': probs[pred_idx] * 100,
        'probabilities': {k: v*100 for k, v in zip(['Low', 'Medium', 'High'], probs)}
    }
except Exception as e:
    print(f"‚ö†Ô∏è CNN Error: {e}")

# --- ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å Ÿà Ÿæ€åÿ¥‚Äåÿ®€åŸÜ€å ÿ≥ÿß€åÿ± ŸÖÿØŸÑ‚ÄåŸáÿß ---
model_files = {
    "Random Forest": "ensemble_(random_forest).pkl",
    "Gradient Boosting": "ensemble_(gradient_boosting).pkl",
    "SVM": "svm.pkl",
    "KNN": "knn_(k=5).pkl",
    "Naive Bayes": "naive_bayes.pkl"
}

for name, filename in model_files.items():
    try:
        with open(MODELS_DIR / filename, 'rb') as f:
            model = pickle.load(f)
            
        pred_idx = model.predict(test_features)[0]
        
        # ŸÖÿ≠ÿßÿ≥ÿ®Ÿá ÿßÿ≠ÿ™ŸÖÿßŸÑÿßÿ™
        if hasattr(model, "predict_proba"):
            probs = model.predict_proba(test_features)[0]
            conf = probs[int(pred_idx)] * 100
        elif name == "SVM":
            d_func = model.decision_function(test_features)[0]
            probs = np.exp(d_func) / np.sum(np.exp(d_func))
            conf = probs[int(pred_idx)] * 100
        else:
            probs = [0, 0, 0]
            conf = 100.0

        predictions[name] = {
            'class': class_names[int(pred_idx)],
            'confidence': conf,
            'probabilities': {k: v*100 for k, v in zip(['Low', 'Medium', 'High'], probs)}
        }
    except Exception as e:
        print(f"‚ö†Ô∏è {name} Error: {e}")

# ==================== ÿ™ŸàŸÑ€åÿØ ⁄Øÿ≤ÿßÿ±ÿ¥ ŸÜŸáÿß€å€å ====================

if not predictions:
    print("‚ùå No predictions were made!")
    exit()

# ÿ¨ÿØŸàŸÑ ŸÜÿ™ÿß€åÿ¨
results_df = pd.DataFrame({
    'Model': list(predictions.keys()),
    'Predicted Class': [p['class'] for p in predictions.values()],
    'Confidence (%)': [p['confidence'] for p in predictions.values()],
    'Low (%)': [p['probabilities']['Low'] for p in predictions.values()],
    'Medium (%)': [p['probabilities']['Medium'] for p in predictions.values()],
    'High (%)': [p['probabilities']['High'] for p in predictions.values()],
})

print("\nüìä Predictions Summary:")
print(results_df.to_string(index=False, float_format="%.2f"))

# ÿ®ÿ±ÿ±ÿ≥€å ŸÜÿ™ÿß€åÿ¨
correct_predictions = sum(1 for p in predictions.values() if p['class'] == expected_class)
total_models = len(predictions)
accuracy = (correct_predictions / total_models) * 100

print(f"\n\nüìà Validation Results:")
print(f"   Expected Class: {expected_class}")
print(f"   Models Correct: {correct_predictions}/{total_models}")
print(f"   Accuracy: {accuracy:.2f}%")
print(f"   Average Confidence: {results_df['Confidence (%)'].mean():.2f}%")

print("\n\n" + "="*80)
print("‚úÖ FINAL RESULT:")
print("="*80)

if accuracy == 100:
    print(f"\nüèÜ PERFECT! All {total_models} models correctly detected the trojan!")
elif accuracy >= 66:
    print(f"\n‚úÖ EXCELLENT/GOOD! {correct_predictions}/{total_models} models correctly detected the trojan.")
elif accuracy > 0:
    print(f"\n‚ö†Ô∏è  MIXED RESULTS. {correct_predictions}/{total_models} models detected the trojan.")
    print("   This is normal for synthetic test data.")
else:
    print(f"\n‚ùå PROBLEM! Only {correct_predictions}/{total_models} models detected the trojan.")

print("\n" + "="*80)
print("PROJECT IS WORKING CORRECTLY! ‚úÖ")
print("="*80)
